{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DocQA","text":""},{"location":"notebook/docqa/","title":"Tutorial on DocQA","text":"In\u00a0[1]: Copied! <pre>from docqa.config import Settings\nfrom docqa.pipeline.engine import QAEngine\n</pre> from docqa.config import Settings from docqa.pipeline.engine import QAEngine In\u00a0[2]: Copied! <pre>settings = Settings(\n        llm_provider=\"ollama\",\n        llm_model=\"qwen2.5:7b\",\n        embed_provider=\"ollama\",\n        embed_model=\"nomic-embed-text\",\n        faiss_index_dir=\"./.local/faiss_index\",\n        retrieval_type=\"mmr\",\n        retrieval_k=4,\n    )\n</pre> settings = Settings(         llm_provider=\"ollama\",         llm_model=\"qwen2.5:7b\",         embed_provider=\"ollama\",         embed_model=\"nomic-embed-text\",         faiss_index_dir=\"./.local/faiss_index\",         retrieval_type=\"mmr\",         retrieval_k=4,     ) <p>Validate that the value provided for the settings are correct</p> In\u00a0[3]: Copied! <pre>settings.validate()\n</pre> settings.validate() In\u00a0[4]: Copied! <pre>engine = QAEngine(settings)\n</pre> engine = QAEngine(settings) <p>We will ingest the pdf document first</p> In\u00a0[5]: Copied! <pre>ingest_stats = engine.ingest_pdf(\"../../documents/soc2-type2.pdf\")\nprint(\"Ingest:\", ingest_stats)\n</pre> ingest_stats = engine.ingest_pdf(\"../../documents/soc2-type2.pdf\") print(\"Ingest:\", ingest_stats) <pre>Ingest: {'ingested_pages': 56, 'chunks_added': 193, 'index_dir': './.local/faiss_index'}\n</pre> <p>Next we will ingest the sample json</p> In\u00a0[6]: Copied! <pre>ingest_stats = engine.ingest_json(\"../../documents/sample_json.json\")\nprint(\"Ingest:\", ingest_stats)\n</pre> ingest_stats = engine.ingest_json(\"../../documents/sample_json.json\") print(\"Ingest:\", ingest_stats) <pre>Ingest: {'ingested_pages': 19, 'chunks_added': 19, 'index_dir': './.local/faiss_index'}\n</pre> In\u00a0[7]: Copied! <pre>result = engine.answer(\"Which cloud providers do you rely on?\")\n</pre> result = engine.answer(\"Which cloud providers do you rely on?\") In\u00a0[9]: Copied! <pre>print(\"\\nAnswer:\", result[\"answer\"])\nprint(\"\\nTop source:\", result[\"sources\"][0] if result[\"sources\"] else None)\n</pre> print(\"\\nAnswer:\", result[\"answer\"]) print(\"\\nTop source:\", result[\"sources\"][0] if result[\"sources\"] else None) <pre>\nAnswer: Amazon Web Services Inc. (AWS)\n\nTop source: {'source': '../../documents/soc2-type2.pdf', 'source_type': 'pdf', 'page': 4, 'chunk_index': 4, 'doc_id': None, 'text_snippet': '2  INDEPENDENT SERVICE AUDITOR\u2019S REPORT    To Board of Directors  Product Fruits s.r.o.    Scope    We have examined the accompanying \u201c Description of Product Fruits , a cloud -hosted software application \u201d  provided by Product Fruits s.r.o. throughout the period July 24, 2024 to July 23, 2025  (the description) and the  suitability of the design and operating effectiveness of controls to meet Product Fruits s.r.o. \u2019s service  commitments and system requirements based on the criteria for Security, Confidentiality, Availability, Processing  Integrity &amp; Privacy principles set forth in TSP Section  100 Principles and Criteria, Trust Services Principles and  Criteria for Security, Confidentiality and Availability (applicable trust services criteria) throughout the period July  24, 2024 to July 23, 2025.    Product Fruits s.r.o.  uses Amazon Web Services Inc. (AWS), a subservice organization, to provide cloud', 'score': 0.0}\n</pre>"},{"location":"notebook/docqa/#using-docqa","title":"Using DocQA\u00b6","text":"<p>Test out the functionality for the document answering python package</p>"},{"location":"notebook/docqa/#set-the-config","title":"Set the config\u00b6","text":""},{"location":"notebook/docqa/#question-answer-engine","title":"Question-Answer Engine\u00b6","text":""},{"location":"notebook/mvp/","title":"Basic Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>import json\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_ollama import ChatOllama, OllamaEmbeddings\n\nimport time\n</pre> import json from pathlib import Path from typing import List, Tuple  from langchain_core.documents import Document from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_community.vectorstores import FAISS from langchain_ollama import ChatOllama, OllamaEmbeddings  import time In\u00a0[2]: Copied! <pre>def convert_csv_to_json(file):\n    df = pd.read_csv(f\"../../documents/{file}.csv\", index_col=0)\n    df = df.fillna(\"\")\n    records = df.to_dict(orient=\"records\")\n    with open(f\"../../documents/{file}.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(records, f, indent=2, ensure_ascii=False)\n</pre> def convert_csv_to_json(file):     df = pd.read_csv(f\"../../documents/{file}.csv\", index_col=0)     df = df.fillna(\"\")     records = df.to_dict(orient=\"records\")     with open(f\"../../documents/{file}.json\", \"w\", encoding=\"utf-8\") as f:         json.dump(records, f, indent=2, ensure_ascii=False) In\u00a0[3]: Copied! <pre># convert_csv_to_json(\"sample_json\")\n</pre> # convert_csv_to_json(\"sample_json\") <p>Ollama Config</p> In\u00a0[4]: Copied! <pre>LLM_MODEL = \"qwen2.5:7b\"\nEMBED_MODEL = \"nomic-embed-text\"\nTEMPERATURE = 0.0\n</pre> LLM_MODEL = \"qwen2.5:7b\" EMBED_MODEL = \"nomic-embed-text\" TEMPERATURE = 0.0 <p>Retriever Config</p> In\u00a0[5]: Copied! <pre>FAISS_INDEX_DIR = Path(\"./faiss_index\")\nCHUNK_SIZE = 1000\nCHUNK_OVERLAP = 200\nTOP_K = 6\n</pre> FAISS_INDEX_DIR = Path(\"./faiss_index\") CHUNK_SIZE = 1000 CHUNK_OVERLAP = 200 TOP_K = 6 In\u00a0[6]: Copied! <pre>from langchain_community.document_loaders import PyPDFLoader\n</pre> from langchain_community.document_loaders import PyPDFLoader In\u00a0[7]: Copied! <pre>def load_pdf(path: str) -&gt; list[Document]:\n    \"\"\"Loads a pdf\"\"\"\n    loader = PyPDFLoader(path)\n    pages = loader.load()\n    return pages\n</pre> def load_pdf(path: str) -&gt; list[Document]:     \"\"\"Loads a pdf\"\"\"     loader = PyPDFLoader(path)     pages = loader.load()     return pages In\u00a0[8]: Copied! <pre>def load_json(path: str) -&gt; List[Document]:\n    \"\"\"Loads a json\"\"\"\n    data = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n    docs: List[Document] = []\n\n    for item in data:\n        text = (\n            f\"Question: {item.get('question','')}\\n\"\n            f\"Answer: {item.get('answer','')}\\n\"\n            f\"Comments: {item.get('comments','')}\\n\"\n        )\n        docs.append(\n            Document(\n                page_content=text,\n                metadata={\"id\": item.get(\"id\"), \"source\": path}\n            )\n        )\n    return docs\n</pre> def load_json(path: str) -&gt; List[Document]:     \"\"\"Loads a json\"\"\"     data = json.loads(Path(path).read_text(encoding=\"utf-8\"))     docs: List[Document] = []      for item in data:         text = (             f\"Question: {item.get('question','')}\\n\"             f\"Answer: {item.get('answer','')}\\n\"             f\"Comments: {item.get('comments','')}\\n\"         )         docs.append(             Document(                 page_content=text,                 metadata={\"id\": item.get(\"id\"), \"source\": path}             )         )     return docs In\u00a0[9]: Copied! <pre>def split_documents(\n    docs: List[Document],\n    chunk_size: int = CHUNK_SIZE,\n    overlap: int = CHUNK_OVERLAP,\n) -&gt; List[Document]:\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=overlap,\n        add_start_index=True,\n    )\n    return splitter.split_documents(docs)\n</pre> def split_documents(     docs: List[Document],     chunk_size: int = CHUNK_SIZE,     overlap: int = CHUNK_OVERLAP, ) -&gt; List[Document]:     splitter = RecursiveCharacterTextSplitter(         chunk_size=chunk_size,         chunk_overlap=overlap,         add_start_index=True,     )     return splitter.split_documents(docs) In\u00a0[10]: Copied! <pre>embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n</pre> embeddings = OllamaEmbeddings(model=EMBED_MODEL) In\u00a0[11]: Copied! <pre>def build_or_load_faiss(\n    chunks: List[Document],\n    index_dir: Path = FAISS_INDEX_DIR,\n) -&gt; FAISS:\n    \"\"\"Create FAISS index if missing; otherwise load and add documents.\"\"\"\n    if index_dir.exists():\n        vs = FAISS.load_local(\n            str(index_dir),\n            embeddings,\n            allow_dangerous_deserialization=True,\n        )\n        vs.add_documents(chunks)\n    else:\n        vs = FAISS.from_documents(chunks, embeddings)\n\n    vs.save_local(str(index_dir))\n    return vs\n</pre> def build_or_load_faiss(     chunks: List[Document],     index_dir: Path = FAISS_INDEX_DIR, ) -&gt; FAISS:     \"\"\"Create FAISS index if missing; otherwise load and add documents.\"\"\"     if index_dir.exists():         vs = FAISS.load_local(             str(index_dir),             embeddings,             allow_dangerous_deserialization=True,         )         vs.add_documents(chunks)     else:         vs = FAISS.from_documents(chunks, embeddings)      vs.save_local(str(index_dir))     return vs In\u00a0[12]: Copied! <pre>def retrieve_with_scores(\n    vector_store: FAISS,\n    query: str,\n    k: int = TOP_K,\n) -&gt; List[Tuple[Document, float]]:\n    return vector_store.similarity_search_with_score(query, k=k)\n\n\ndef format_context(docs_and_scores: List[Tuple[Document, float]]) -&gt; str:\n    \"\"\"\n        Creates the context from the retrieved sources by appending metadata and citation for the downstream LLM to answer\n    \"\"\"\n    parts = []\n    for i, (doc, score) in enumerate(docs_and_scores, 1):\n        meta = doc.metadata or {}\n        cite = []\n        \n        if meta.get(\"source_type\") == \"pdf\":\n            if \"page\" in meta:\n                cite.append(f\"page={meta['page']}\")\n        \n        if meta.get(\"source_type\") == \"json\" and meta.get(\"id\"):\n            cite.append(f\"id={meta['id']}\")\n        \n        cite_str = \", \".join(cite) if cite else \"no-meta\"\n        \n        parts.append(\n            f\"[{i}] ({cite_str}, score={score:.4f})\\n{doc.page_content}\"\n        )\n\n    return \"\\n\\n\".join(parts)\n</pre> def retrieve_with_scores(     vector_store: FAISS,     query: str,     k: int = TOP_K, ) -&gt; List[Tuple[Document, float]]:     return vector_store.similarity_search_with_score(query, k=k)   def format_context(docs_and_scores: List[Tuple[Document, float]]) -&gt; str:     \"\"\"         Creates the context from the retrieved sources by appending metadata and citation for the downstream LLM to answer     \"\"\"     parts = []     for i, (doc, score) in enumerate(docs_and_scores, 1):         meta = doc.metadata or {}         cite = []                  if meta.get(\"source_type\") == \"pdf\":             if \"page\" in meta:                 cite.append(f\"page={meta['page']}\")                  if meta.get(\"source_type\") == \"json\" and meta.get(\"id\"):             cite.append(f\"id={meta['id']}\")                  cite_str = \", \".join(cite) if cite else \"no-meta\"                  parts.append(             f\"[{i}] ({cite_str}, score={score:.4f})\\n{doc.page_content}\"         )      return \"\\n\\n\".join(parts) In\u00a0[13]: Copied! <pre>llm = ChatOllama(model=LLM_MODEL, temperature=TEMPERATURE)\n</pre> llm = ChatOllama(model=LLM_MODEL, temperature=TEMPERATURE) In\u00a0[14]: Copied! <pre>def answer_one(query: str, context: str) -&gt; str:\n    prompt = f\"\"\"You are a security and compliance documentation assistant.\n\nAnswer the question using ONLY the information below.\nRules:\n- Do NOT use external knowledge.\n- Do NOT guess or infer.\n- If the answer is not clearly present, respond exactly with: Data not found.\n\nInformation:\n{context}\n\nQuestion:\n{query}\n\nAnswer:\n\"\"\"\n    resp = llm.invoke(prompt)\n    return resp.content.strip()\n</pre> def answer_one(query: str, context: str) -&gt; str:     prompt = f\"\"\"You are a security and compliance documentation assistant.  Answer the question using ONLY the information below. Rules: - Do NOT use external knowledge. - Do NOT guess or infer. - If the answer is not clearly present, respond exactly with: Data not found.  Information: {context}  Question: {query}  Answer: \"\"\"     resp = llm.invoke(prompt)     return resp.content.strip()  In\u00a0[15]: Copied! <pre>t0 = time.time()\n\npdf_docs = load_pdf(\"../../documents/soc2-type2.pdf\")\njson_docs = load_json(\"../../documents/sample_json.json\")\n\nall_docs = pdf_docs + json_docs\nsplits = split_documents(all_docs)\n\nvector_store = build_or_load_faiss(splits)\n\nprint(f\"Loaded docs: pdf={len(pdf_docs)}, json={len(json_docs)}\")\nprint(f\"Chunks: {len(splits)}\")\nprint(f\"Index ready. Elapsed: {time.time() - t0:.2f}s\")\n</pre> t0 = time.time()  pdf_docs = load_pdf(\"../../documents/soc2-type2.pdf\") json_docs = load_json(\"../../documents/sample_json.json\")  all_docs = pdf_docs + json_docs splits = split_documents(all_docs)  vector_store = build_or_load_faiss(splits)  print(f\"Loaded docs: pdf={len(pdf_docs)}, json={len(json_docs)}\") print(f\"Chunks: {len(splits)}\") print(f\"Index ready. Elapsed: {time.time() - t0:.2f}s\") <pre>Loaded docs: pdf=56, json=19\nChunks: 212\nIndex ready. Elapsed: 6.03s\n</pre> In\u00a0[16]: Copied! <pre>retriever = vector_store.as_retriever(\n    search_type=\"similarity\", \n    search_kwargs={\"k\": 50}\n)\n</pre> retriever = vector_store.as_retriever(     search_type=\"similarity\",      search_kwargs={\"k\": 50} ) In\u00a0[17]: Copied! <pre>query = \"Which cloud providers do you rely on?\"\n</pre> query = \"Which cloud providers do you rely on?\" In\u00a0[18]: Copied! <pre>docs_and_scores = retrieve_with_scores(vector_store, query, k=TOP_K)\ncontext = format_context(docs_and_scores)\n</pre> docs_and_scores = retrieve_with_scores(vector_store, query, k=TOP_K) context = format_context(docs_and_scores) In\u00a0[19]: Copied! <pre>print(context[:2000])\n</pre> print(context[:2000]) <pre>[1] (no-meta, score=0.7132)\n2 \nINDEPENDENT SERVICE AUDITOR\u2019S REPORT \n \nTo Board of Directors \nProduct Fruits s.r.o. \n \nScope \n \nWe have examined the accompanying \u201c Description of Product Fruits , a cloud -hosted software application \u201d \nprovided by Product Fruits s.r.o. throughout the period July 24, 2024 to July 23, 2025  (the description) and the \nsuitability of the design and operating effectiveness of controls to meet Product Fruits s.r.o. \u2019s service \ncommitments and system requirements based on the criteria for Security, Confidentiality, Availability, Processing \nIntegrity &amp; Privacy principles set forth in TSP Section  100 Principles and Criteria, Trust Services Principles and \nCriteria for Security, Confidentiality and Availability (applicable trust services criteria) throughout the period July \n24, 2024 to July 23, 2025. \n \nProduct Fruits s.r.o.  uses Amazon Web Services Inc. (AWS), a subservice organization, to provide cloud\n\n[2] (no-meta, score=0.7132)\n2 \nINDEPENDENT SERVICE AUDITOR\u2019S REPORT \n \nTo Board of Directors \nProduct Fruits s.r.o. \n \nScope \n \nWe have examined the accompanying \u201c Description of Product Fruits , a cloud -hosted software application \u201d \nprovided by Product Fruits s.r.o. throughout the period July 24, 2024 to July 23, 2025  (the description) and the \nsuitability of the design and operating effectiveness of controls to meet Product Fruits s.r.o. \u2019s service \ncommitments and system requirements based on the criteria for Security, Confidentiality, Availability, Processing \nIntegrity &amp; Privacy principles set forth in TSP Section  100 Principles and Criteria, Trust Services Principles and \nCriteria for Security, Confidentiality and Availability (applicable trust services criteria) throughout the period July \n24, 2024 to July 23, 2025. \n \nProduct Fruits s.r.o.  uses Amazon Web Services Inc. (AWS), a subservice organization, to provide cloud\n\n[3] (no-meta, score=0.7132)\n2 \nINDEPENDENT SERVICE AUDITOR\u2019S REPORT \n \nTo Board of Directors \nProduct Frui\n</pre> In\u00a0[20]: Copied! <pre>ans = answer_one(query, context)\n</pre> ans = answer_one(query, context) In\u00a0[21]: Copied! <pre>print(ans)\n</pre> print(ans) <pre>Product Fruits s.r.o. relies on Amazon Web Services Inc. (AWS) as a cloud provider.\n</pre>"},{"location":"notebook/mvp/#docqa-mvp","title":"DocQA MVP\u00b6","text":"<p>I created this notebook to demonstrate the core Retrieval-Augmented Generation (RAG) workflow that became the foundation of the <code>DocQA</code> package. I made a deliberate decision to use Ollama during experimentation so I could iterate quickly without spending OpenAI credits. The components are designed to be modular, so I can easily swap Ollama for OpenAI (or any other provider) later by changing the LLM and embedding implementations\u2014without rewriting the pipeline.</p> <p>Topics</p> <ol> <li>Load documents (PDF + JSON)</li> <li>Split into chunks</li> <li>Embed chunks and store in a Vector DB (FAISS)</li> <li>Retrieve top\u2011K similar chunks for a question</li> <li>Generate an answer with an LLM (Ollama)</li> </ol> <p>Local Setup for Ollama</p> <p>I run Ollama locally and pull the models I need:</p> <pre>ollama serve\nollama pull qwen2.5:7b\nollama pull nomic-embed-text\n</pre> <p>Models used :</p> <ul> <li><code>qwen2.5:7b</code> \u2192 answer generation</li> <li><code>nomic-embed-text</code> \u2192 embeddings for vector search</li> </ul>"},{"location":"notebook/mvp/#helper-function","title":"Helper Function\u00b6","text":""},{"location":"notebook/mvp/#config","title":"Config\u00b6","text":""},{"location":"notebook/mvp/#document-loaders","title":"Document Loaders\u00b6","text":""},{"location":"notebook/mvp/#chunking","title":"Chunking\u00b6","text":""},{"location":"notebook/mvp/#creating-embeddings-vector-store","title":"Creating Embeddings + Vector Store\u00b6","text":""},{"location":"notebook/mvp/#retrieval-helpers","title":"Retrieval Helpers\u00b6","text":""},{"location":"pages/api/","title":"API Endpoints","text":"Category Method Endpoint Description Health Check GET <code>/health</code> Returns API health status Document Management POST <code>/ingest</code> Upload and index PDF or JSON documents Question Answering POST <code>/answer</code> Answer a single question Question Answering POST <code>/answer/batch</code> Upload JSON file with questions array or"},{"location":"pages/api/#swagger-testing","title":"Swagger Testing","text":"<p>Run the following command from <code>docqa-api</code> <pre><code>poetry run uvicorn docqa_api.api.main:app --reload\n</code></pre></p> <p>Then visit <code>http://localhost:8000/docs</code> to interact with the API.</p> <p>You will be able to view and interact with the API </p> <p></p>"},{"location":"pages/api/#ingest","title":"Ingest","text":"<p>We can ingest a PDF file and check the output. </p> <p>The API returned a 200 OK response, indicating that the file was successfully processed and added to the FAISS vector database. </p>"},{"location":"pages/api/#answer","title":"Answer","text":"<p>There are 2 APIs for answering a question. The first one is mostly a test api which returns the answer along with other metadata information about the retreval process such as the chunk index, process score etc. </p> <p>For the current use case we will look at the <code>/answer_batch</code> api which returns the answers to a batch of questions asked in the form of a json file.  </p> <p>The response is in the form of a question, answer pair </p>"},{"location":"pages/architecture/","title":"Architecture","text":""},{"location":"pages/user-requirement/","title":"User Requirements","text":"<p>This API is a document-based Question Answering service that allows users to upload a document (PDF or JSON) that serves as a knowledge base. The API processes the document, converts its content into searchable embeddings using a vector database, and answers each question strictly based on the document\u2019s information. It returns the results as a JSON object mapping each question to its corresponding answer, enabling automated analysis, compliance checks, and knowledge extraction from documents.</p>"},{"location":"pages/user-requirement/#user-roles","title":"User Roles","text":"<p>User roles refers to the people who would be interacting with the API. We will define 2 roles : </p> <ul> <li><code>Developer</code>: A user who uploads the documents to form the knowledge base.  </li> <li><code>Consumer</code>: A user who would be asking question by passing the json objects.  </li> </ul>"},{"location":"pages/user-requirement/#functional-requirements","title":"Functional Requirements","text":""},{"location":"pages/user-requirement/#1-inputs","title":"1. Inputs","text":"<ul> <li>Accept PDF and JSON files only; reject all other formats with HTTP 415.</li> <li>Enforce a configurable maximum file size (default: 50 MB); return HTTP 413 if exceeded.</li> <li>Extract and normalize text from PDF and JSON inputs.</li> </ul>"},{"location":"pages/user-requirement/#2-text-processing","title":"2. Text Processing","text":"<ul> <li>Chunk text using configurable chunk size and overlap.</li> <li>Preserve minimal metadata per chunk:</li> <li><code>doc_id</code></li> <li><code>page</code> (if applicable)</li> <li><code>chunk_index</code></li> </ul>"},{"location":"pages/user-requirement/#3-embeddings-storage","title":"3. Embeddings &amp; Storage","text":"<ul> <li>Generate embeddings using a configurable provider (<code>OpenAI</code>).</li> <li>Store embeddings in a Vector Database (<code>FAISS</code>).</li> <li>Persist chunk metadata alongside each vector for traceability.</li> </ul>"},{"location":"pages/user-requirement/#4-retrieval-answering","title":"4. Retrieval &amp; Answering","text":"<ul> <li>Perform top-k semantic retrieval per question.</li> <li>Generate answers using a Retrieval-Augmented Generation (RAG) pipeline with a large language model.</li> <li>Answers must be grounded strictly in retrieved content by citing the source.</li> </ul>"}]}